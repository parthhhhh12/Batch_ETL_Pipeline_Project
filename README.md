ğŸ“Š Batch ETL Pipeline using PySpark on Azure Databricks
ğŸ“– Project Overview

This project demonstrates the design and implementation of a batch ETL pipeline using PySpark on Azure Databricks.

The pipeline processes the NYC Taxi Trip dataset from raw CSV files into cleaned, structured, and aggregated Parquet files, which are stored in Azure Blob Storage for efficient analytics and reporting.

ğŸ¯ Objectives

ğŸ“¥ Read raw CSV data from Azure Blob Storage

ğŸ§¹ Perform data cleaning, filtering, and type casting

ğŸ“Š Apply aggregations (e.g., total rides per vendor per day)

ğŸ’¾ Store final results in Parquet format back to Azure Blob Storage

ğŸ› ï¸ Tools & Technologies

ğŸ”¹ Azure Databricks

ğŸ”¹ PySpark

ğŸ”¹ Azure Blob Storage

ğŸ”¹ Parquet File Format

ğŸ”¹ DBML (for ER Diagram)

ğŸ“‚ Dataset

Source: NYC Taxi Trip Data â€“ Kaggle

Schema Highlights:

VendorID

pickup_datetime, dropoff_datetime

passenger_count, trip_distance

pickup_location_id, dropoff_location_id

fare_amount, tip_amount, total_amount

ğŸ”„ ETL Pipeline Steps

1ï¸âƒ£ Ingest Raw Data â†’ Load CSV from Azure Blob into Databricks

2ï¸âƒ£ Data Cleaning & Transformation â†’ Type casting + filtering invalid rows using PySpark

3ï¸âƒ£ Aggregation â†’ Compute total rides per vendor per day

4ï¸âƒ£ Load Processed Data â†’ Write aggregated dataset in Parquet format back to Azure Blob Storage

ğŸ—‚ï¸ Data Model

RawTripData â†’ Raw input table (direct from CSV)

CleanedTripData â†’ Filtered & type-casted dataset

AggregatedTrips â†’ Summary dataset (rides per vendor per day)

âœ… Conclusion

This project demonstrates how PySpark on Azure Databricks can:

âš¡ Process large-scale datasets in the cloud

ğŸ”’ Ensure reliability, scalability, and efficiency

ğŸ“ˆ Deliver analytics-ready data in Parquet format for BI & reporting tools
