# ğŸ“Š Batch ETL Pipeline using PySpark on Azure Databricks  

---

## ğŸ“– Project Overview  

This project demonstrates the design and implementation of a **batch ETL pipeline** using **PySpark in Azure Databricks**.  

The pipeline processes the **NYC Taxi Trip dataset** from raw CSV into cleaned, structured, and aggregated datasets.  
The final output is stored in **Parquet format** within **Azure Blob Storage** for efficient analytics and reporting.  

---

## ğŸ¯ Objectives  

- ğŸ“¥ Ingest raw CSV data from **Azure Blob Storage**  
- ğŸ§¹ Perform **data cleaning, filtering, and type casting**  
- ğŸ“Š Apply **aggregations** (e.g., total rides per vendor per day)  
- ğŸ’¾ Store the final results in **Parquet format** back into Azure Blob Storage  

---

## ğŸ› ï¸ Tools & Technologies  

- ğŸ”¹ **Azure Databricks**  
- ğŸ”¹ **PySpark**  
- ğŸ”¹ **Azure Blob Storage**  
- ğŸ”¹ **Parquet File Format**  
- ğŸ”¹ **DBML** (for ER Diagram)  

---

## ğŸ“‚ Dataset  

**Source:** [NYC Taxi Trip Data â€“ Kaggle](https://www.kaggle.com/datasets/anandaramg/taxi-trip-data-nyc)  

**Schema Highlights:**  

- `VendorID`  
- `pickup_datetime`, `dropoff_datetime`  
- `passenger_count`, `trip_distance`  
- `pickup_location_id`, `dropoff_location_id`  
- `payment details`, `fare_amount`, `tip_amount`, `total_amount`  

---

## ğŸ”„ ETL Pipeline Steps  

1ï¸âƒ£ **Ingest Raw Data** â†’ Mount or access Azure Blob Storage to read the CSV file  

2ï¸âƒ£ **Read Data** â†’ Use PySpark DataFrame API with `inferSchema` and `header` options  

3ï¸âƒ£ **Data Cleaning & Transformation** â†’ Apply `.withColumn()` for type casting + filter invalid rows  

4ï¸âƒ£ **Aggregation** â†’ Compute total rides per vendor per day using `groupBy()` and `count()`  

5ï¸âƒ£ **Load Processed Data** â†’ Store final aggregated data in **Parquet format** back into Azure Blob Storage  

6ï¸âƒ£ **Verification** â†’ Validate results in Azure Cloud Storage  

---

## ğŸ—‚ï¸ Data Model (ER Diagram)  

- **RawTripData** â†’ Raw input table with all fields from CSV  
- **CleanedTripData** â†’ Filtered and type-casted dataset  
- **AggregatedTrips** â†’ Aggregated dataset (rides per vendor per day)  

**Relationships:**  

- CleanedTripData is derived from RawTripData  
- AggregatedTrips references CleanedTripData  

---

## ğŸ“œ Code Overview  

The PySpark code performs the following operations inside **Databricks Notebook cells**:  

- Read CSV file into DataFrame  
- Clean and type-cast columns  
- Filter invalid rows  
- Aggregate trips per vendor per day  
- Write results in **Parquet format**  

---

## âœ… Conclusion  

This ETL project successfully demonstrates how to:  

- âš¡ Use **PySpark on Azure Databricks** for scalable data processing  
- ğŸ§¹ Perform **data cleaning, transformation, and aggregation**  
- ğŸ’¾ Store processed datasets in **Parquet format** for efficient analytics  

The pipeline ensures **scalability, reliability, and performance**, while making data ready for BI and reporting tools.  

---

## ğŸ“‚ Repository Contents  

- `README.md` â†’ Project documentation (this file)  
- `Parth_Batch_ETL_Pipeline.docx` â†’ ğŸ“¥ **Download this file to view the complete detailed project report**  

---

